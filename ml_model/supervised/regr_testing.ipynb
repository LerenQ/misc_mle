{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/qianleren/anaconda3/envs/dissertation/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "    \n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "data = load_boston()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 13)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = data.data[:, [3]]\n",
    "X = data.data[:, [0,5,6,12]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([i[0] for i in Y if i[0]==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.3200e-03, 6.5750e+00, 6.5200e+01, 4.9800e+00],\n",
       "       [2.7310e-02, 6.4210e+00, 7.8900e+01, 9.1400e+00],\n",
       "       [2.7290e-02, 7.1850e+00, 6.1100e+01, 4.0300e+00],\n",
       "       ...,\n",
       "       [6.0760e-02, 6.9760e+00, 9.1000e+01, 5.6400e+00],\n",
       "       [1.0959e-01, 6.7940e+00, 8.9300e+01, 6.4800e+00],\n",
       "       [4.7410e-02, 6.0300e+00, 8.0800e+01, 7.8800e+00]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(d, seed=1):\n",
    "    '''\n",
    "    Function to initialize the parameters for the logisitic regression model\n",
    "    \n",
    "    Inputs:\n",
    "        d: number of features for every data point\n",
    "        seed: random generator seed for reproducing the results\n",
    "        \n",
    "    Outputs:\n",
    "        w: weight vector of dimensions (d, 1)\n",
    "        b: scalar bias value\n",
    "    '''\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # NOTE: initialize w to be a (d,1) column vector instead of (d,) vector \n",
    "    # Hint: initialize w to a random vector with small values. For example, 0.01*np.random.randn(.) can be used.\n",
    "    #       and initialize b to scalar 0\n",
    "    # your code here\n",
    "    w = 0.01*np.random.randn(d, 1)\n",
    "    b = 0\n",
    "    \n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    # your code here\n",
    "    A = 1/(1+ np.exp(-z))\n",
    "    \n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_loss(A,Y):\n",
    "    '''\n",
    "    Function to calculate the logistic loss given the predictions and the targets.\n",
    "    \n",
    "    Inputs:\n",
    "        A: Estimated prediction values, A is of dimension (1, m)\n",
    "        Y: groundtruth labels, Y is of dimension (1, m)\n",
    "        \n",
    "    Outputs:\n",
    "        loss: logistic loss\n",
    "    '''\n",
    "    m = A.shape[1]\n",
    "    # your code here\n",
    "    loss = (-1/m) * np.sum(Y*A+ (1-Y)*np.log(1-A))\n",
    "#     print((-1/m))\n",
    "#     print(np.dot((Y),np.log(A).T)+ np.dot((1-Y),np.log(1-A).T) )\n",
    "#     print(np.dot((Y),np.log(A).T), Y, np.log(A), A)\n",
    "#     print(np.dot((1-Y),np.log(1-A).T) )\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_fn(X,dZ):\n",
    "    '''\n",
    "    Function to calculate the gradients of weights (dw) and biases (db) w.r.t the objective function L.\n",
    "    \n",
    "    Inputs:\n",
    "        X: training data of dimensions (d, m)\n",
    "        dZ: gradient dL/dZ where L is the logistic loss and Z = w^T*X+b is the input to the sigmoid activation function\n",
    "            dZ is of dimensions (1, m)\n",
    "        \n",
    "    outputs:\n",
    "        dw: gradient dL/dw - gradient of the weight w.r.t. the logistic loss. It is of dimensions (d,1)\n",
    "        db: gradient dL/db - gradient of the bias w.r.t. the logistic loss. It is a scalar\n",
    "    '''\n",
    "    m = X.shape[1]\n",
    "    # your code here\n",
    "    dw = (1/m)* np.dot(X, dZ.T)\n",
    "    db = (1/m)*np.sum(dZ)\n",
    "    \n",
    "    return dw,db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fit(w,b,X,Y,alpha,n_epochs,log=False):\n",
    "    '''\n",
    "    Function to fit a logistic model with the parameters w,b to the training data with labels X and Y.\n",
    "    \n",
    "    Inputs:\n",
    "        w: weight vector of dimensions (d, 1)\n",
    "        b: scalar bias value\n",
    "        X: training data of dimensions (d, m)\n",
    "        Y: training data labels of dimensions (1, m)\n",
    "        alpha: learning rate\n",
    "        n_epochs: number of epochs to train the model\n",
    "        \n",
    "    Outputs:\n",
    "        params: a dictionary to hold parameters w and b\n",
    "        losses: a list train loss at every epoch\n",
    "    '''\n",
    "    losses=[]\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        # Implement the steps in the logistic regression using the functions defined earlier.\n",
    "        # For each iteration of the for loop\n",
    "            # Step 1: Calculate output Z = w.T*X + b\n",
    "            # Step 2: Apply sigmoid activation: A = sigmoid(Z)\n",
    "            # Step 3: Calculate loss = logistic_loss(.) between predicted values A and groundtruth labels Y\n",
    "            # Step 4: Estimate gradient dZ = A-Y\n",
    "            # Step 5: Estimate gradients dw and db using grad_fn(.).\n",
    "            # Step 6: Update parameters w and b using gradients dw, db and learning rate\n",
    "            #         w = w - alpha * dw\n",
    "            #         b = b - alpha * db\n",
    "\n",
    "        # your code here\n",
    "        Z = np.dot(w.T,X) + b\n",
    "        A = sigmoid(Z)\n",
    "        loss = logistic_loss(A,Y)\n",
    "        dZ = A - Y\n",
    "        dw,db = grad_fn(X,dZ)\n",
    "        w = w - alpha * dw\n",
    "        b = b - alpha * db\n",
    "        \n",
    "        if epoch%100 == 0:\n",
    "            losses.append(loss)\n",
    "            if log == True:\n",
    "                print(\"After %i iterations, Loss = %f\"%(epoch,loss))\n",
    "    params ={\"w\":w,\"b\":b}\n",
    "    \n",
    "    return params,losses    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict(params,X,Y=np. array([]),pred_threshold=0.5):\n",
    "    '''\n",
    "    Function to calculate category predictions on given data and returns the accuracy of the predictions.\n",
    "    Inputs:\n",
    "        params: a dictionary to hold parameters w and b\n",
    "        X: training data of dimensions (d, m)\n",
    "        Y: training data labels of dimensions (1, m). If not provided, the function merely makes predictions on X\n",
    "        \n",
    "    outputs:\n",
    "        Y_Pred: Predicted class labels for X. Has dimensions (1, m)\n",
    "        acc: accuracy of prediction over X if Y is provided else, 0 \n",
    "        loss: loss of prediction over X if Y is provided else, Inf  \n",
    "    '''\n",
    "    w = params['w']\n",
    "    b = params['b']\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # Calculate Z using X, w and b\n",
    "    # Calculate A using the sigmoid - A is the set of (1,m) probabilities\n",
    "    # Calculate the prediction labels Y_Pred of size (1,m) using A and pred_threshold \n",
    "    # When A>pred_threshold Y_Pred is 1 else 0\n",
    "    # your code here\n",
    "    Z = np.dot(w.T,X) + b\n",
    "    A = sigmoid(Z)\n",
    "    Y_Pred = np.zeros(A.shape)\n",
    "    Y_Pred[A>=pred_threshold] = 1\n",
    "  \n",
    "    acc = 0\n",
    "    loss = float('inf')\n",
    "    if Y.size!=0:\n",
    "        loss = logistic_loss(A,Y)\n",
    "        acc = np.mean(Y_Pred==Y)\n",
    "    return Y_Pred, acc, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.001\n",
    "n_epochs = 2000\n",
    "\n",
    "# Write code to initialize parameters w and b with initialize(.) (use train_X to get feature dimensions d)\n",
    "# Use model_fit(.) to estimate the updated 'params' of the logistic regression model and calculate how the 'losses' varies \n",
    "# Use variables 'params' and 'losses' to store the outputs of model_fit(.) \n",
    "# your code here\n",
    "X0 = X.T\n",
    "Y0 = Y.T\n",
    "d = X0.shape[0]\n",
    "w,b = initialize(d, seed=100)\n",
    "params,losses = model_fit(w,b,X0,Y0,alpha,n_epochs,log=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9921305644548102,\n",
       " 0.1129128026104857,\n",
       " 0.10500179836980615,\n",
       " 0.09892968419038174,\n",
       " 0.09416163153916518,\n",
       " 0.09034552020878654,\n",
       " 0.08724047299020152,\n",
       " 0.08467688370991742,\n",
       " 0.0825325619379671,\n",
       " 0.08071776483512903,\n",
       " 0.07916546155418605,\n",
       " 0.07782482651743701,\n",
       " 0.07665679376860804,\n",
       " 0.07563096161723638,\n",
       " 0.07472340098836268,\n",
       " 0.07391507984052702,\n",
       " 0.07319071454341419,\n",
       " 0.07253792164232313,\n",
       " 0.07194658392122749,\n",
       " 0.07140837134243674]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'w': array([[-0.02540775],\n",
       "        [-0.28045047],\n",
       "        [ 0.01317362],\n",
       "        [-0.12897213]]),\n",
       " 'b': -0.04513705226906909}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.3200e-03, 6.5750e+00, 6.5200e+01, 4.9800e+00],\n",
       "       [2.7310e-02, 6.4210e+00, 7.8900e+01, 9.1400e+00],\n",
       "       [2.7290e-02, 7.1850e+00, 6.1100e+01, 4.0300e+00],\n",
       "       ...,\n",
       "       [6.0760e-02, 6.9760e+00, 9.1000e+01, 5.6400e+00],\n",
       "       [1.0959e-01, 6.7940e+00, 8.9300e+01, 6.4800e+00],\n",
       "       [4.7410e-02, 6.0300e+00, 8.0800e+01, 7.8800e+00]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "16a9694dd653c3e07703c83cdecd20aed57b27dbfad5ccadf9e094ddea6b0413"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('dissertation')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
